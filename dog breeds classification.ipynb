{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os, cv2, random, time, shutil, csv\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tqdm import tqdm\n",
    "np.random.seed(42)\n",
    "%matplotlib inline \n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Dense, GlobalAveragePooling2D, Lambda, Dropout, InputLayer, Input\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout, InputLayer, Lambda, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Paths\n",
    "train_dir = '/kaggle/input/dog-breed-identification/train'\n",
    "test_dir = '/kaggle/input/dog-breed-identification/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000bec180eb18c7604dcecc8fe0dba07</td>\n",
       "      <td>boston_bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001513dfcb2ffafc82cccf4d8bbaba97</td>\n",
       "      <td>dingo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001cdf01b096e06d78e9e5112d419397</td>\n",
       "      <td>pekinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00214f311d5d2247d5dfe4fe24b2303d</td>\n",
       "      <td>bluetick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0021f9ceb3235effd7fcde7f7538ed62</td>\n",
       "      <td>golden_retriever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id             breed\n",
       "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull\n",
       "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo\n",
       "2  001cdf01b096e06d78e9e5112d419397          pekinese\n",
       "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick\n",
       "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read train labels.\n",
    "labels_dataframe = pd.read_csv('/kaggle/input/dog-breed-identification/labels.csv')\n",
    "#Read sample_submission file to be modified by pridected labels.\n",
    "sample_df = pd.read_csv('/kaggle/input/dog-breed-identification/sample_submission.csv')\n",
    "#Incpect labels_dataframe.\n",
    "labels_dataframe.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of alphabetically sorted labels.\n",
    "dog_breeds = sorted(list(set(labels_dataframe['breed'])))\n",
    "n_classes = len(dog_breeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map each label string to an integer label.\n",
    "class_to_num = dict(zip(dog_breeds, range(n_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_array(data_dir, labels_dataframe, img_size = (224,224,3)):\n",
    "\n",
    "    images_names = labels_dataframe['id']\n",
    "    images_labels = labels_dataframe['breed']\n",
    "    data_size = len(images_names)\n",
    "    #initailize output arrays.\n",
    "    X = np.zeros([data_size, img_size[0], img_size[1], img_size[2]], dtype=np.uint8)\n",
    "    y = np.zeros([data_size,1], dtype=np.uint8)\n",
    "    #read data and lables.\n",
    "    for i in tqdm(range(data_size)):\n",
    "        image_name = images_names[i]\n",
    "        img_dir = os.path.join(data_dir, image_name+'.jpg')\n",
    "        img_pixels = load_img(img_dir, target_size=img_size)\n",
    "        X[i] = img_pixels\n",
    "        \n",
    "        image_breed = images_labels[i]\n",
    "        y[i] = class_to_num[image_breed]\n",
    "    \n",
    "    #One hot encoder\n",
    "    y = to_categorical(y)\n",
    "    #shuffle    \n",
    "    ind = np.random.permutation(data_size)\n",
    "    X = X[ind]\n",
    "    y = y[ind]\n",
    "    print('Ouptut Data Size: ', X.shape)\n",
    "    print('Ouptut Label Size: ', y.shape)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10222/10222 [01:01<00:00, 167.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ouptut Data Size:  (10222, 331, 331, 3)\n",
      "Ouptut Label Size:  (10222, 120)\n"
     ]
    }
   ],
   "source": [
    "#img_size chosen to be 331 to suit the used architectures.\n",
    "img_size = (331,331,3)\n",
    "X, y = images_to_array(train_dir, labels_dataframe, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(model_name, data_preprocessor, input_size, data):\n",
    "    base_model = model_name(weights='imagenet', include_top=False,\n",
    "                            input_shape=input_size)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape = input_size))\n",
    "    model.add(Lambda(preprocess_input))\n",
    "    model.add(base_model)\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    \n",
    "    feature_maps = model.predict(data)\n",
    "    \n",
    "    print('Feature maps shape: ', feature_maps.shape)\n",
    "    return feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature maps shape:  (10222, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Extract features using InceptionV3 as extractor.\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "inception_preprocessor = preprocess_input\n",
    "inception_features = get_features(InceptionV3,inception_preprocessor,img_size, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 1s 0us/step\n",
      "Feature maps shape:  (10222, 512)\n"
     ]
    }
   ],
   "source": [
    "# Extract features using VGG16 as extractor.\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "vgg16_preprocessor = preprocess_input\n",
    "vgg16_features = get_features(VGG16,vgg16_preprocessor,img_size, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 1s 0us/step\n",
      "Feature maps shape:  (10222, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Extract features using ResNet50 as extractor.\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "resnet_preprocessor = preprocess_input\n",
    "resnet_features = get_features(ResNet50,resnet_preprocessor,img_size, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "EarlyStop_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "my_callback=[EarlyStop_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9199 samples, validate on 1023 samples\n",
      "Epoch 1/60\n",
      "9199/9199 [==============================] - 0s 45us/step - loss: 22.2606 - accuracy: 0.0186 - val_loss: 6.1951 - val_accuracy: 0.1105\n",
      "Epoch 2/60\n",
      "9199/9199 [==============================] - 0s 44us/step - loss: 14.9255 - accuracy: 0.0592 - val_loss: 3.4658 - val_accuracy: 0.2893\n",
      "Epoch 3/60\n",
      "9199/9199 [==============================] - 0s 52us/step - loss: 10.6113 - accuracy: 0.1259 - val_loss: 2.4979 - val_accuracy: 0.4418\n",
      "Epoch 4/60\n",
      "9199/9199 [==============================] - 0s 45us/step - loss: 8.2150 - accuracy: 0.1821 - val_loss: 1.8953 - val_accuracy: 0.5396\n",
      "Epoch 5/60\n",
      "9199/9199 [==============================] - 0s 33us/step - loss: 6.4752 - accuracy: 0.2398 - val_loss: 1.6701 - val_accuracy: 0.5748\n",
      "Epoch 6/60\n",
      "9199/9199 [==============================] - 0s 34us/step - loss: 5.3893 - accuracy: 0.2917 - val_loss: 1.4895 - val_accuracy: 0.6002\n",
      "Epoch 7/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 4.6395 - accuracy: 0.3271 - val_loss: 1.3493 - val_accuracy: 0.6354\n",
      "Epoch 8/60\n",
      "9199/9199 [==============================] - 0s 28us/step - loss: 4.0778 - accuracy: 0.3624 - val_loss: 1.3356 - val_accuracy: 0.6657\n",
      "Epoch 9/60\n",
      "9199/9199 [==============================] - 0s 26us/step - loss: 3.7013 - accuracy: 0.3850 - val_loss: 1.2665 - val_accuracy: 0.6657\n",
      "Epoch 10/60\n",
      "9199/9199 [==============================] - 0s 26us/step - loss: 3.3408 - accuracy: 0.4143 - val_loss: 1.2149 - val_accuracy: 0.6745\n",
      "Epoch 11/60\n",
      "9199/9199 [==============================] - 0s 26us/step - loss: 3.2207 - accuracy: 0.4173 - val_loss: 1.1358 - val_accuracy: 0.6813\n",
      "Epoch 12/60\n",
      "9199/9199 [==============================] - 0s 26us/step - loss: 2.9106 - accuracy: 0.4477 - val_loss: 1.1888 - val_accuracy: 0.6901\n",
      "Epoch 13/60\n",
      "9199/9199 [==============================] - 0s 26us/step - loss: 2.7370 - accuracy: 0.4634 - val_loss: 1.1405 - val_accuracy: 0.6921\n",
      "Epoch 14/60\n",
      "9199/9199 [==============================] - 0s 25us/step - loss: 2.6377 - accuracy: 0.4666 - val_loss: 1.1173 - val_accuracy: 0.6862\n",
      "Epoch 15/60\n",
      "9199/9199 [==============================] - 0s 31us/step - loss: 2.5405 - accuracy: 0.4769 - val_loss: 1.1102 - val_accuracy: 0.6833\n",
      "Epoch 16/60\n",
      "9199/9199 [==============================] - 0s 25us/step - loss: 2.4114 - accuracy: 0.4895 - val_loss: 1.0743 - val_accuracy: 0.6901\n",
      "Epoch 17/60\n",
      "9199/9199 [==============================] - 0s 26us/step - loss: 2.3753 - accuracy: 0.4903 - val_loss: 1.1039 - val_accuracy: 0.7028\n",
      "Epoch 18/60\n",
      "9199/9199 [==============================] - 0s 26us/step - loss: 2.3579 - accuracy: 0.4959 - val_loss: 1.1130 - val_accuracy: 0.6989\n",
      "Epoch 19/60\n",
      "9199/9199 [==============================] - 0s 27us/step - loss: 2.2580 - accuracy: 0.5066 - val_loss: 1.1111 - val_accuracy: 0.6979\n",
      "Epoch 20/60\n",
      "9199/9199 [==============================] - 0s 27us/step - loss: 2.2546 - accuracy: 0.5032 - val_loss: 1.0896 - val_accuracy: 0.6950\n",
      "Epoch 21/60\n",
      "9199/9199 [==============================] - 0s 26us/step - loss: 2.1748 - accuracy: 0.5036 - val_loss: 1.1201 - val_accuracy: 0.6784\n",
      "Epoch 22/60\n",
      "9199/9199 [==============================] - 0s 31us/step - loss: 2.2031 - accuracy: 0.5049 - val_loss: 1.0905 - val_accuracy: 0.6940\n",
      "Epoch 23/60\n",
      "9199/9199 [==============================] - 0s 26us/step - loss: 2.1448 - accuracy: 0.5099 - val_loss: 1.0580 - val_accuracy: 0.6970\n",
      "Epoch 24/60\n",
      "9199/9199 [==============================] - 0s 29us/step - loss: 2.1058 - accuracy: 0.5106 - val_loss: 1.0675 - val_accuracy: 0.6921\n",
      "Epoch 25/60\n",
      "9199/9199 [==============================] - 0s 28us/step - loss: 2.1011 - accuracy: 0.5152 - val_loss: 1.0499 - val_accuracy: 0.6970\n",
      "Epoch 26/60\n",
      "9199/9199 [==============================] - 0s 27us/step - loss: 2.1309 - accuracy: 0.5106 - val_loss: 1.0193 - val_accuracy: 0.7077\n",
      "Epoch 27/60\n",
      "9199/9199 [==============================] - 0s 26us/step - loss: 2.0629 - accuracy: 0.5206 - val_loss: 1.0892 - val_accuracy: 0.6960\n",
      "Epoch 28/60\n",
      "9199/9199 [==============================] - 0s 28us/step - loss: 2.0802 - accuracy: 0.5136 - val_loss: 1.0710 - val_accuracy: 0.6989\n",
      "Epoch 29/60\n",
      "9199/9199 [==============================] - 0s 27us/step - loss: 2.0466 - accuracy: 0.5142 - val_loss: 1.0422 - val_accuracy: 0.6931\n",
      "Epoch 30/60\n",
      "9199/9199 [==============================] - 0s 28us/step - loss: 2.0572 - accuracy: 0.5161 - val_loss: 1.0700 - val_accuracy: 0.6911\n",
      "Epoch 31/60\n",
      "9199/9199 [==============================] - 0s 26us/step - loss: 2.0673 - accuracy: 0.5148 - val_loss: 1.0568 - val_accuracy: 0.6960\n",
      "Epoch 32/60\n",
      "9199/9199 [==============================] - 0s 28us/step - loss: 2.0677 - accuracy: 0.5253 - val_loss: 1.0495 - val_accuracy: 0.7028\n",
      "Epoch 33/60\n",
      "9199/9199 [==============================] - 0s 32us/step - loss: 2.0519 - accuracy: 0.5184 - val_loss: 1.0466 - val_accuracy: 0.7019\n",
      "Epoch 34/60\n",
      "9199/9199 [==============================] - 0s 27us/step - loss: 2.0698 - accuracy: 0.5148 - val_loss: 1.0593 - val_accuracy: 0.6911\n",
      "Epoch 35/60\n",
      "9199/9199 [==============================] - 0s 26us/step - loss: 2.0496 - accuracy: 0.5170 - val_loss: 1.0495 - val_accuracy: 0.6931\n",
      "Epoch 36/60\n",
      "9199/9199 [==============================] - 0s 26us/step - loss: 2.0740 - accuracy: 0.5196 - val_loss: 1.0330 - val_accuracy: 0.7019\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape = (vgg16_features.shape[1:] )))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(120, activation = 'softmax'))\n",
    "    \n",
    "model.compile(loss = 'categorical_crossentropy', optimizer ='Adam', metrics = ['accuracy'])\n",
    "h=model.fit(vgg16_features, y,\n",
    "            batch_size=128,\n",
    "            epochs=60,\n",
    "            validation_split=0.1,\n",
    "            callbacks=my_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9199 samples, validate on 1023 samples\n",
      "Epoch 1/60\n",
      "9199/9199 [==============================] - 0s 42us/step - loss: 4.0943 - accuracy: 0.1344 - val_loss: 2.1389 - val_accuracy: 0.5973\n",
      "Epoch 2/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 2.1481 - accuracy: 0.4390 - val_loss: 1.3216 - val_accuracy: 0.7234\n",
      "Epoch 3/60\n",
      "9199/9199 [==============================] - 0s 31us/step - loss: 1.5176 - accuracy: 0.6007 - val_loss: 1.0335 - val_accuracy: 0.7498\n",
      "Epoch 4/60\n",
      "9199/9199 [==============================] - 0s 31us/step - loss: 1.1988 - accuracy: 0.6747 - val_loss: 0.8798 - val_accuracy: 0.7693\n",
      "Epoch 5/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 1.0309 - accuracy: 0.7139 - val_loss: 0.7978 - val_accuracy: 0.7947\n",
      "Epoch 6/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.8832 - accuracy: 0.7521 - val_loss: 0.7313 - val_accuracy: 0.8016\n",
      "Epoch 7/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.7862 - accuracy: 0.7775 - val_loss: 0.6999 - val_accuracy: 0.8104\n",
      "Epoch 8/60\n",
      "9199/9199 [==============================] - 0s 48us/step - loss: 0.7188 - accuracy: 0.7945 - val_loss: 0.6659 - val_accuracy: 0.7986\n",
      "Epoch 9/60\n",
      "9199/9199 [==============================] - 0s 39us/step - loss: 0.6673 - accuracy: 0.8095 - val_loss: 0.6397 - val_accuracy: 0.8084\n",
      "Epoch 10/60\n",
      "9199/9199 [==============================] - 0s 29us/step - loss: 0.6075 - accuracy: 0.8247 - val_loss: 0.6247 - val_accuracy: 0.8152\n",
      "Epoch 11/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.5879 - accuracy: 0.8313 - val_loss: 0.6092 - val_accuracy: 0.8065\n",
      "Epoch 12/60\n",
      "9199/9199 [==============================] - 0s 31us/step - loss: 0.5518 - accuracy: 0.8397 - val_loss: 0.6055 - val_accuracy: 0.8016\n",
      "Epoch 13/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.5162 - accuracy: 0.8489 - val_loss: 0.5976 - val_accuracy: 0.8065\n",
      "Epoch 14/60\n",
      "9199/9199 [==============================] - 0s 29us/step - loss: 0.4940 - accuracy: 0.8529 - val_loss: 0.5952 - val_accuracy: 0.8104\n",
      "Epoch 15/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.4715 - accuracy: 0.8579 - val_loss: 0.5771 - val_accuracy: 0.8192\n",
      "Epoch 16/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.4479 - accuracy: 0.8666 - val_loss: 0.5953 - val_accuracy: 0.8006\n",
      "Epoch 17/60\n",
      "9199/9199 [==============================] - 0s 29us/step - loss: 0.4460 - accuracy: 0.8681 - val_loss: 0.5681 - val_accuracy: 0.8192\n",
      "Epoch 18/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.4296 - accuracy: 0.8728 - val_loss: 0.5832 - val_accuracy: 0.8065\n",
      "Epoch 19/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.4236 - accuracy: 0.8684 - val_loss: 0.5776 - val_accuracy: 0.8162\n",
      "Epoch 20/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.4091 - accuracy: 0.8754 - val_loss: 0.5667 - val_accuracy: 0.8035\n",
      "Epoch 21/60\n",
      "9199/9199 [==============================] - 0s 29us/step - loss: 0.3814 - accuracy: 0.8848 - val_loss: 0.5700 - val_accuracy: 0.8065\n",
      "Epoch 22/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.3655 - accuracy: 0.8911 - val_loss: 0.5769 - val_accuracy: 0.8143\n",
      "Epoch 23/60\n",
      "9199/9199 [==============================] - 0s 29us/step - loss: 0.3676 - accuracy: 0.8890 - val_loss: 0.5632 - val_accuracy: 0.8104\n",
      "Epoch 24/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.3684 - accuracy: 0.8898 - val_loss: 0.5689 - val_accuracy: 0.8016\n",
      "Epoch 25/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.3481 - accuracy: 0.8899 - val_loss: 0.5674 - val_accuracy: 0.8045\n",
      "Epoch 26/60\n",
      "9199/9199 [==============================] - 0s 32us/step - loss: 0.3388 - accuracy: 0.8956 - val_loss: 0.5726 - val_accuracy: 0.7977\n",
      "Epoch 27/60\n",
      "9199/9199 [==============================] - 0s 29us/step - loss: 0.3324 - accuracy: 0.8972 - val_loss: 0.5728 - val_accuracy: 0.7967\n",
      "Epoch 28/60\n",
      "9199/9199 [==============================] - 0s 29us/step - loss: 0.3310 - accuracy: 0.8981 - val_loss: 0.5456 - val_accuracy: 0.8143\n",
      "Epoch 29/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.3258 - accuracy: 0.9021 - val_loss: 0.5598 - val_accuracy: 0.8035\n",
      "Epoch 30/60\n",
      "9199/9199 [==============================] - 0s 29us/step - loss: 0.3196 - accuracy: 0.9023 - val_loss: 0.5675 - val_accuracy: 0.8074\n",
      "Epoch 31/60\n",
      "9199/9199 [==============================] - 0s 33us/step - loss: 0.3133 - accuracy: 0.9011 - val_loss: 0.5626 - val_accuracy: 0.8006\n",
      "Epoch 32/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.3116 - accuracy: 0.9033 - val_loss: 0.5576 - val_accuracy: 0.8094\n",
      "Epoch 33/60\n",
      "9199/9199 [==============================] - 0s 29us/step - loss: 0.3105 - accuracy: 0.9038 - val_loss: 0.5688 - val_accuracy: 0.8084\n",
      "Epoch 34/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.3098 - accuracy: 0.9039 - val_loss: 0.5667 - val_accuracy: 0.8231\n",
      "Epoch 35/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.3064 - accuracy: 0.9035 - val_loss: 0.5685 - val_accuracy: 0.8104\n",
      "Epoch 36/60\n",
      "9199/9199 [==============================] - 0s 29us/step - loss: 0.2881 - accuracy: 0.9065 - val_loss: 0.5717 - val_accuracy: 0.8104\n",
      "Epoch 37/60\n",
      "9199/9199 [==============================] - 0s 29us/step - loss: 0.2883 - accuracy: 0.9031 - val_loss: 0.5576 - val_accuracy: 0.8123\n",
      "Epoch 38/60\n",
      "9199/9199 [==============================] - 0s 29us/step - loss: 0.2824 - accuracy: 0.9116 - val_loss: 0.5900 - val_accuracy: 0.8094\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape = (resnet_features.shape[1:] )))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(120, activation = 'softmax'))\n",
    "    \n",
    "model.compile(loss = 'categorical_crossentropy', optimizer ='Adam', metrics = ['accuracy'])\n",
    "h=model.fit(resnet_features, y,\n",
    "            batch_size=128,\n",
    "            epochs=60,\n",
    "            validation_split=0.1,\n",
    "            callbacks=my_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9199 samples, validate on 1023 samples\n",
      "Epoch 1/60\n",
      "9199/9199 [==============================] - 0s 42us/step - loss: 2.8423 - accuracy: 0.4745 - val_loss: 1.0319 - val_accuracy: 0.8759\n",
      "Epoch 2/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.7968 - accuracy: 0.8498 - val_loss: 0.4823 - val_accuracy: 0.9042\n",
      "Epoch 3/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.4946 - accuracy: 0.8846 - val_loss: 0.3779 - val_accuracy: 0.9062\n",
      "Epoch 4/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.3886 - accuracy: 0.9019 - val_loss: 0.3424 - val_accuracy: 0.9071\n",
      "Epoch 5/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.3367 - accuracy: 0.9085 - val_loss: 0.3116 - val_accuracy: 0.9198\n",
      "Epoch 6/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.3032 - accuracy: 0.9124 - val_loss: 0.3058 - val_accuracy: 0.9140\n",
      "Epoch 7/60\n",
      "9199/9199 [==============================] - 0s 45us/step - loss: 0.2776 - accuracy: 0.9196 - val_loss: 0.2969 - val_accuracy: 0.9091\n",
      "Epoch 8/60\n",
      "9199/9199 [==============================] - 0s 37us/step - loss: 0.2516 - accuracy: 0.9213 - val_loss: 0.2924 - val_accuracy: 0.9150\n",
      "Epoch 9/60\n",
      "9199/9199 [==============================] - 0s 32us/step - loss: 0.2326 - accuracy: 0.9300 - val_loss: 0.2884 - val_accuracy: 0.9189\n",
      "Epoch 10/60\n",
      "9199/9199 [==============================] - 0s 31us/step - loss: 0.2205 - accuracy: 0.9337 - val_loss: 0.2850 - val_accuracy: 0.9130\n",
      "Epoch 11/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.2077 - accuracy: 0.9385 - val_loss: 0.2824 - val_accuracy: 0.9169\n",
      "Epoch 12/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.1991 - accuracy: 0.9387 - val_loss: 0.2881 - val_accuracy: 0.9120\n",
      "Epoch 13/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.1911 - accuracy: 0.9396 - val_loss: 0.2859 - val_accuracy: 0.9150\n",
      "Epoch 14/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.1785 - accuracy: 0.9451 - val_loss: 0.2848 - val_accuracy: 0.9130\n",
      "Epoch 15/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.1696 - accuracy: 0.9454 - val_loss: 0.2881 - val_accuracy: 0.9101\n",
      "Epoch 16/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.1598 - accuracy: 0.9500 - val_loss: 0.2876 - val_accuracy: 0.9101\n",
      "Epoch 17/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.1585 - accuracy: 0.9493 - val_loss: 0.2782 - val_accuracy: 0.9179\n",
      "Epoch 18/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.1553 - accuracy: 0.9510 - val_loss: 0.2835 - val_accuracy: 0.9110\n",
      "Epoch 19/60\n",
      "9199/9199 [==============================] - 0s 29us/step - loss: 0.1503 - accuracy: 0.9510 - val_loss: 0.2829 - val_accuracy: 0.9120\n",
      "Epoch 20/60\n",
      "9199/9199 [==============================] - 0s 29us/step - loss: 0.1477 - accuracy: 0.9543 - val_loss: 0.2880 - val_accuracy: 0.9150\n",
      "Epoch 21/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.1480 - accuracy: 0.9524 - val_loss: 0.2897 - val_accuracy: 0.9140\n",
      "Epoch 22/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.1390 - accuracy: 0.9565 - val_loss: 0.2891 - val_accuracy: 0.9120\n",
      "Epoch 23/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.1343 - accuracy: 0.9595 - val_loss: 0.2935 - val_accuracy: 0.9071\n",
      "Epoch 24/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.1265 - accuracy: 0.9588 - val_loss: 0.2836 - val_accuracy: 0.9140\n",
      "Epoch 25/60\n",
      "9199/9199 [==============================] - 0s 30us/step - loss: 0.1224 - accuracy: 0.9628 - val_loss: 0.2929 - val_accuracy: 0.9140\n",
      "Epoch 26/60\n",
      "9199/9199 [==============================] - 0s 31us/step - loss: 0.1186 - accuracy: 0.9627 - val_loss: 0.2944 - val_accuracy: 0.9120\n",
      "Epoch 27/60\n",
      "9199/9199 [==============================] - 0s 29us/step - loss: 0.1226 - accuracy: 0.9589 - val_loss: 0.2903 - val_accuracy: 0.9130\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape = (inception_features.shape[1:] )))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(120, activation = 'softmax'))\n",
    "    \n",
    "model.compile(loss = 'categorical_crossentropy', optimizer ='Adam', metrics = ['accuracy'])\n",
    "h=model.fit(inception_features, y,\n",
    "            batch_size=128,\n",
    "            epochs=60,\n",
    "            validation_split=0.1,\n",
    "            callbacks=my_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10357/10357 [00:48<00:00, 213.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ouptut Data Size:  (10357, 331, 331, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def images_to_array2(data_dir, labels_dataframe, img_size = (224,224,3)):\n",
    "   \n",
    "    images_names = labels_dataframe['id']\n",
    "    data_size = len(images_names)\n",
    "    X = np.zeros([data_size, img_size[0], img_size[1], 3], dtype=np.uint8)\n",
    "    \n",
    "    for i in tqdm(range(data_size)):\n",
    "        image_name = images_names[i]\n",
    "        img_dir = os.path.join(data_dir, image_name+'.jpg')\n",
    "        img_pixels = tf.keras.preprocessing.image.load_img(img_dir, target_size=img_size)\n",
    "        X[i] = img_pixels\n",
    "        \n",
    "    print('Ouptut Data Size: ', X.shape)\n",
    "    return X\n",
    "\n",
    "test_data = images_to_array2(test_dir, sample_df, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract test data features.\n",
    "inception_features = get_features(InceptionV3, inception_preprocessor, img_size, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Free up some space.\n",
    "del test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict test labels given test data features.\n",
    "y_pred = model.predict(inception_features, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create submission file\n",
    "for b in dog_breeds:\n",
    "    sample_df[b] = y_pred[:,class_to_num[b]]\n",
    "sample_df.to_csv('pred.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
